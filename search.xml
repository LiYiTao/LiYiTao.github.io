<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[caffe Blob]]></title>
    <url>%2F2018%2F02%2F05%2Fcaffe-Blob%2F</url>
    <content type="text"><![CDATA[概述Caffe使用成为Blob的4维数组用于存储和交换数据。Blob提供了统一的存储器接口，持有一批图像或者其他数据，权值，权值更新值。Blob在内存中表示4维数组，维度从低到高为(width_,height_,channels_,num_) 四个维度分别表示，（宽，长，通道数，图像数）。 例子：Feature Map例如(224,224,3,2)就表示两个张图，每张图有RGB三通道，每张图片的大小是224*224。同样，卷积核也是这样表示的 存储Blob是保存为一维数组的，存储方式是 行 列 通道 图像就是一行一行取出来拍成一维数组，取完一个通道的添加通道，取完一张图的添加图。]]></content>
      <tags>
        <tag>Faster-RCNN</tag>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Faster-RCNN移植到PYNQ]]></title>
    <url>%2F2018%2F02%2F05%2FFaster-RCNN%E7%A7%BB%E6%A4%8D%E5%88%B0PYNQ%2F</url>
    <content type="text"><![CDATA[整体思路 安装依赖库，主要openCV和caffe，因为openCV没有用到计算的函数，直接用pip的软件包，caffe的话到时要全改成FPGA的，这里还是移植了caffe的 主体程序放到DAC/Python里，目标检测程序参考demo.py修改到Su.ipynb中（关键是调用im_detect），python2改成python3。 修改对应的图片路径，并把输出改成比赛的要求，参考Su.ipynb。 注意这里的caffe是Faster-RCNN自带的caffe，有特殊的层，在caffe-fast-rcnn下，需要编译 编译caffe安装依赖库1234sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compilersudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-devsudo apt-get install libatlas-base-dev 因为PYNQ板子镜像是ubuntu15.10wily系统，处理器架构是armhf，现在已经不提供支持了（各种404 Not Found），就要换apt的源。找了好久发现http://old-releases.ubuntu.com，ubuntu的旧版本库可以用。 替换源/etc/apt/sources.list.d/目录下面，又list文件，deb，deb-src两个前面改成http://old-releases/ubuntu ，即可以替换成新的源。 网上很多教程说是/etc/apt/目录下的sources.list文件，但是找不到这个文件，不知道怎么回事。 替换完源后直接安装还会又unable to locate package得错误，因为没有应用新的源，执行sudo apt-get update就可以正常安装了。 Makefile.config复制Makefile.config.example重命名为Makefile.config，再修改配置参数。123make all -j2make test make runtest -j2表示用两个核来编译，因为只有两个 遇到的错误 Makefile:563: recipe for target ‘.build_release/src/caffe/util/hdf5.o’ failed 12INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/arm-linux-gnueabihf/hdf5/serial /usr/lib/arm-linux-gnueabihf/hdf5 因为这是arm核的，PC可以是64位或x86什么的。 /usr/bin/ld: cannot find -lopencv_imgcodecs opencv版本弄错了，OPENCV_VERSION := 2，我是2.4.9改成2。 /usr/bin/ld: cannot find -lboost_python3 boost_python是C++和python的接口，在/usr/lib/arm-linux-gnueabihf目录下找libboost_python3.so，发现只有libboost_python-py34.so，所以修改Makefile下的PYTHON_LIBRARIES := boost_python3为PYTHON_LIBRARIES := boost_python-py34 make test遇到错误src/caffe/test/test_smooth_L1_loss_layer.cpp:11:35: fatal error: caffe/vision_layers.hpp: 没有那个文件或目录compilation terminated.Makefile:563: recipe for target ‘.build_release/src/caffe/test/test_smooth_L1_loss_layer.o’ failed 找到 caffe-fast-rcnn/src/caffe/test/test_smooth_L1_loss_layer.cpp，将里面的#include &quot;caffe/vision_layers.hpp&quot;注释掉 找到 caffe-fast-rcnn/src/caffe/test/test_smooth_L1_loss_layer.cpp 和 caffe-fast-rcnn/src/caffe/test/test_roi_pooling_layer.cpp，去掉最前面的typedef ::testing::Types&lt;GPUDevice&lt;float&gt;, GPUDevice&lt;double&gt; &gt; TestDtypesGPU;，并将 TestDtypesGPU改为 TestDtypesAndDevices。因为是用CPU版本，找的好辛苦。 最后的Makefile.config123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).# USE_CUDNN := 1# CPU-only switch (uncomment to build without GPU support).# 选择CPU模式CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you&apos;re using OpenCV 3# opencv版本OPENCV_VERSION := 2#USE_OPENCV := 1# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# &quot;sudo apt-get install nvidia-cuda-toolkit&quot; then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 lines for compatibility.#CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \# -gencode arch=compute_20,code=sm_21 \# -gencode arch=compute_30,code=sm_30 \# -gencode arch=compute_35,code=sm_35 \# -gencode arch=compute_50,code=sm_50 \# -gencode arch=compute_50,code=compute_50# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlas# BLAS选择为atlas，要安装BLAS := atlas# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /usr/include/atlas-x86_64-base# BLAS_LIB := /usr/lib64/atlas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.# MATLAB_DIR := /usr/local# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.#PYTHON_INCLUDE := /usr/include/python2.7 \# /usr/lib/python2.7/dist-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it&apos;s in root.# ANACONDA_HOME := $(HOME)/anaconda# PYTHON_INCLUDE := $(ANACONDA_HOME)/include \ # $(ANACONDA_HOME)/include/python2.7 \ # $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include \# Uncomment to use Python 3 (default is Python 2)# 库路径和头文件路径，include要包含Python.h和numpy/arrayobject.h PYTHON_LIBRARIES := boost_python-py34 python3.4 PYTHON_INCLUDE := /usr/include/python3.4 \ /usr/include \ /usr/lib/python3.4/dist-packages/numpy/core/include# We need to be able to find libpythonX.X.so or .dylib.PYTHON_LIB := /usr/lib/python3.4/config-3.4m-arm-linux-gnueabihf# PYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c &apos;import numpy.core; print(numpy.core.__file__)&apos;))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs) WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.# 其他需要的路径，注意要加入hdf5的路径INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serialLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/arm-linux-gnueabihf/hdf5/serial /usr/lib/arm-linux-gnueabihf/hdf5# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)# USE_PKG_CONFIG := 1BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that &apos;make runtest&apos; will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ 说明PYNQ里的python版本比较特殊，系统级/usr目录下有python2.7和python3.4但是什么包都没有，pip也没有，用户级/opt目录下有python3.6，是jupyter用的python版本，之前是想安装再python3.6里，一直失败，最后安装再python2.7上成功了，连boost_python的库都不需要了。 编译Python接口在caffe根目录的python文件夹下，有一个requirements.txt的清单文件，上面列出了需要的依赖库，按照这个清单安装就可以了。 安装依赖库1sudo pip install -r python/requirements.txt 就会看到，安装成功的，都会显示Requirement already satisfied, 没有安装成功的，会继续安装。 pip安装包非常慢，不是下载时慢，是setup.py时很慢，一度怀疑卡住了，后来挂在这里一个晚上，把所有的包装好了 pycaffe在caffe根目录执行1make pycaffe 配置py-faster-rcnn 将 ~/py-faster-rcnn/lib/fast_rcnn/config.py的如下内容： 1205 _C.USE_GPU_NMS = False 将 ~/py-faster-rcnn/tools/test_net.py和 ~/py-faster-rcnn/tools/train_net.py的caffe.set_mode_gpu()修改为caffe.set_mode_cpu(). 1280 #caffe.set_mode_gpu()81 caffe.set_mode_cpu() 12101 #caffe.set_mode_gpu()102 caffe.set_mode_cpu() 将~/py-faster-rcnn/lib/setup.py中，含有’nms.gpu_nms’的部分去掉，去掉后的内容如下： 123456789101112131415161718192021112 ext_modules = [113 Extension(114 &quot;utils.cython_bbox&quot;,115 [&quot;utils/bbox.pyx&quot;],116 extra_compile_args=&#123;&apos;gcc&apos;: [&quot;-Wno-cpp&quot;, &quot;-Wno-unused-function&quot;]&#125;,117 include_dirs = [numpy_include]118 ),119 Extension(120 &quot;nms.cpu_nms&quot;,121 [&quot;nms/cpu_nms.pyx&quot;],122 extra_compile_args=&#123;&apos;gcc&apos;: [&quot;-Wno-cpp&quot;, &quot;-Wno-unused-function&quot;]&#125;,123 include_dirs = [numpy_include]124 ),125 Extension(126 &apos;pycocotools._mask&apos;,127 sources=[&apos;pycocotools/maskApi.c&apos;, &apos;pycocotools/_mask.pyx&apos;],128 include_dirs = [numpy_include, &apos;pycocotools&apos;],129 extra_compile_args=&#123;130 &apos;gcc&apos;: [&apos;-Wno-cpp&apos;, &apos;-Wno-unused-function&apos;, &apos;-std=c99&apos;]&#125;,131 ),132 ] 还需要将~/lib/fast_rcnn/nms_wrapper.py的gpu_nms注释掉 129 #from nms.gpu_nms import gpu_nms10 from nms.cpu_nms import cpu_nms 遇到错误 from nms.cpu_nms import cpu_nms失败 因为cpu_nms.pyx导入有问题，新建一个cpu_nms.py，写入如下内容。1234567891011121314151617181920212223242526272829303132333435363738394041424344import numpy as npdef mymax(a, b): return a if a &gt;= b else bdef mymin(a, b): return a if a &lt;= b else bdef cpu_nms(dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] areas = (x2 - x1 + 1) * (y2 - y1 + 1) order = scores.argsort()[::-1] ndets = dets.shape[0] suppressed = np.zeros((ndets), dtype=np.int) keep = [] for _i in range(ndets): i = order[_i] if suppressed[i] == 1: continue keep.append(i) ix1 = x1[i] iy1 = y1[i] ix2 = x2[i] iy2 = y2[i] iarea = areas[i] for _j in range(_i + 1, ndets): j = order[_j] if suppressed[j] == 1: continue xx1 = mymax(ix1, x1[j]) yy1 = mymax(iy1, y1[j]) xx2 = mymin(ix2, x2[j]) yy2 = mymin(iy2, y2[j]) w = mymax(0.0, xx2 - xx1 + 1) h = mymax(0.0, yy2 - yy1 + 1) inter = w * h ovr = inter / (iarea + areas[j] - inter) if ovr &gt;= thresh: suppressed[j] = 1 return keep 运行demo.py执行1python demo.py --cpu 或者1./demo.py --cpu 遇到问题 如有中文注释会出错 要在程序开头添加# -*- coding: utf-8 -*-转换编码方式。 ./demo.py运行怎么选择python的版本 在开头添加#!/usr/bin/python2.7，选到对应版本的python目录。 permission denied 权限不够，切换到root权限，用chmod 777 demo.py设置一下权限即可。]]></content>
      <tags>
        <tag>PYNQ</tag>
        <tag>Faster-RCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYNQ之软硬控制小结]]></title>
    <url>%2F2018%2F01%2F25%2FPYNQ%E4%B9%8B%E8%BD%AF%E7%A1%AC%E6%8E%A7%E5%88%B6%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[overlay流程通过比赛中dma的例子来分析1234567891011overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')dma = overlay.axi_dma_0xlnk = Xlnk()in_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)out_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)dma.sendchannel.transfer(in_buffer)dma.recvchannel.transfer(out_buffer) dma.sendchannel.wait()dma.recvchannel.wait() 1、加载overlayoverlay = Overlay(&#39;/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit&#39;) 调用Overlay的init进行初始化，生成overlay对象。传入的是Su.bit的绝对路径。 tcl = _TCL(_get_tcl_name(self.bitfile_name)) 获取该路径下同名的Su.tcl文件，根据_TCL类生成对象，就是对tcl解析。主要是根据tcl前面的关键字确定后面表示什么意思，会整理出ip_dict等字典（关键信息），具体只要对着代码一步步找。 保存tcl中的ip_dict、gpio_dict、interrupt_controllers、interrupt_pins、hierarchy_dict、clock_dict为overlay的类属性，其中有一个dma的ip核，所以ip_dict中包含以下部分(这就是tcl获取的信息) 1'axi_dma_0': &#123;'phys_addr': 1077936128, 'addr_range': 65536, 'type': 'xilinx.com:ip:axi_dma:7.1', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'axi_dma_0', 'driver': &lt;class 'pynq.lib.dma.DMA'&gt;&#125; 根据ip_dict 和 hierarchy_dict建立_IPMap类对象，作为overlay._ip_map，这里的_IPMap起的作用就是把字典映射成类的属性，下面具体讲。 2、用驱动生成IP对象dma = overlay.axi_dma_0 就会调用overlay的getattr方法， 12345def __getattr__(self, key): if self.is_loaded(): return getattr(self._ip_map, key) else: raise RuntimeError("Overlay not currently loaded") getattr的功能是获取key对应的属性，这里重新定义了，变成获取overlay._ip_map的key属性，这样就会调用_IPMap的getattr。 进而调用_IPMap的getattr， 12345678910111213141516171819202122232425def __getattr__(self, key): if key in self._description['hierarchies']: hierdescription = self._description['hierarchies'][key] hierarchy = hierdescription['driver'](hierdescription) setattr(self, key, hierarchy) return hierarchy elif key in self._description['ip']: ipdescription = self._description['ip'][key] driver = ipdescription['driver'](ipdescription) setattr(self, key, driver) return driver elif key in self._description['interrupts']: interrupt = Interrupt( self._description['interrupts'][key]['fullpath']) setattr(self, key, interrupt) return interrupt elif key in self._description['gpio']: gpio_index = self._description['gpio'][key]['index'] gpio_number = GPIO.get_gpio_pin(gpio_index) gpio = GPIO(gpio_number, 'out') setattr(self, key, gpio) return gpio else: raise AttributeError( "Could not find IP or hierarchy &#123;&#125; in overlay".format(key)) 这里就是判断是属于哪一类的（这里的description是之前初始化_IPMap传入的ip_dict等的打包），比如dma是IP，就会对应获取驱动描述pynq.lib.dma.DMA(这里的驱动描述是ip_dict里的driver)，建立overlay._ip_map.axi_dma_0属性。 setattr(object, name, values) 给对象的属性赋值，若属性不存在，先创建再赋值。 根据驱动描述pynq.lib.dma.DMA类（继承自DefaultIP）创建对应的ip驱动，既生成一个DMA类的对象，赋给overlay._ip_map.axi_dma_0属性，最后把该对象赋给dma，然后dma里通过mmio读写对应的寄存器控制和检测IP核axi_dma_0。 3、自定义驱动 其实这里的DMA类就是自定义驱动了，如果没有自定义驱动则用DefaultIP类做默认驱动，因此所有的自定义驱动都从DefaultIP继承。 DefaultIP主要就是根据IP核的物理地址创建一个mmio类对象，然后封装了一个读和写函数，通过mmio读写寄存器(mmio就是把外围设备映射为内存地址就可以直接读写) 自定义驱动主要就是如何对应的问题，比如为什么能识别出axi_dma_0是DMA类的应该用driver：pynq.lib.dma.DMA（ip_dict中的driver不是tcl自带的，是解析时加上去的），其实是只要在自定义驱动类下加一句绑定，bindto = [&#39;xilinx.com:ip:axi_dma:7.1&#39;] ，这样在tcl解析时就会判断IP核类型，为其加上驱动描述了。 xilinx.com:ip:axi_dma:7.1是ip_dict中的type，是tcl中描述每个IP核类型的值。 mmio的硬件实现1、AXI-LITE总线读写mmio读写寄存器的实质是ZYNQ通过AXI-LITE总线读写IP核，总线具体不详细讲，需要在被控制的IP核上加AXI-LITE的Slaver口，可以用vivado添加会自动生成参考代码，生成4个供读写的寄存器（数量可以自己设置），分别是slv_reg0、slv_reg1、slv_reg2、slv_reg3寄存器，然后这里slv_reg0定义为IP核的基地址，就是根据IP核的基地址创建mmio，mmio.read(0x00)就是读slv_reg0寄存器了，如果读下一个只需要偏置加4，mmio.read(0x00 + 4)。 2、控制多个IP核控制多个IP核其实就是一个Master控制多个Slaver，需要用到AXI-Interconnect，这是一个多功能AXI连接IP核，内部包含很多小的基础核，只要在上面添加一个Master口，再接另外的IP核就可以了，这里关键是配置地址，不同的IP核分配给他的基地址是不一样的，比如我添加了两个DMA，他们的基地址不同，所以mmio.read(0x04)其实是读各自的状态寄存器了（mmio根据基地址建立对象，参数是偏置）]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYNQ之自定义驱动]]></title>
    <url>%2F2018%2F01%2F24%2FPYNQ%E4%B9%8B%E8%87%AA%E5%AE%9A%E4%B9%89%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[自定义驱动 Driver说明当我们自定义了一个新的IP核，在软件中调用时，是用默认的驱动DefaultIP去实例化的，默认驱动只有简单的创建mmio和读写，对于我们需要的功能可能就不友好了。因此我们可以为自定义IP核自定义一个driver。接着之前自定义IP核往下做，之前的IP核什么功能都没有，只有四个寄存器供读写，现在我写一个驱动实现读取（reg0、reg1）两个寄存器里的值，相加并返回（虽然没有一点用而且本末倒置了），来实现添加驱动的实验。 设计要求 自定义驱动要继承自DefaultIP 类的构造函数只能有一个参数description，且通过super来传递 要用bindto绑定到IP的类型 类型的查找方法，在ip_dict中，每个IP核有个type，其对应的值就是要绑定的类型，比如下面的dma中 1'axi_dma_0': &#123;'phys_addr': 1077936128, 'addr_range': 65536, 'type': 'xilinx.com:ip:axi_dma:7.1', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'axi_dma_0', 'driver': &lt;class 'pynq.lib.dma.DMA'&gt;&#125; 可以发现tyep的值为xilinx.com:ip:axi_dma:7.1，所以要在DMA类中添加 bindto = [&#39;xilinx.com:ip:axi_dma:7.1&#39;] 就可以成功绑定，之后发现这种类型的IP核（比如这里的axi_dma_0），就会自动调用DMA来驱动，而不是DefaultIP了 具体实现驱动定义123456789101112from pynq import DefaultIPclass MyDriver(DefaultIP): def __init__(self, description): super().__init__(description=description) bindto = ['xilinx.com:user:tao_axi_lite_ctrl:1.0'] def add(self): a = self.read(0x00) b = self.read(0x00 + 4) return a + b 定义了一个函数，读取两个寄存器的值，相加返回 验证结果123456789101112overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')print(overlay.ip_dict)print(overlay.ip_dict.keys())#help(overlay)my_ip = overlay.tao_axi_lite_ctrl_0#help(my_ip)my_ip.mmio.write(0x00, 1) #写入1到reg0my_ip.mmio.write(0x00 + 4, 2) #写入2到reg1print(my_ip.read(0x00)) #读取reg0得1print(my_ip.read(0x00 + 4)) #读取reg1得2my_ip.add() #调用驱动定义的函数，读取reg0,reg1并相加 分别写入1和2到reg0和reg1，调用my_ip.add函数，得到结果3。同时可以从ip_dict中发现，自定义IP核的驱动描述driver变成了__main__.MyDriver，因为我这个自定义驱动写在了main下。1'tao_axi_lite_ctrl_0': &#123;'phys_addr': 1136656384, 'addr_range': 65536, 'type': 'xilinx.com:user:tao_axi_lite_ctrl:1.0', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'tao_axi_lite_ctrl_0', 'driver': &lt;class '__main__.MyDriver'&gt;&#125;]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AXI-LITE实现控制自定义IP核]]></title>
    <url>%2F2018%2F01%2F24%2FAXI-LITE%E5%AE%9E%E7%8E%B0%E6%8E%A7%E5%88%B6%E8%87%AA%E5%AE%9A%E4%B9%89IP%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[前言这是一个小实验，主要实现自定义一个S_AXI_LITE接口的IP核，然后用PS写入数据该IP核的寄存器，再从该寄存器读取。主要通过mmio来实现读写，关键程序如下：12mmio.write(offset, value)mmio.read(offset) 详解自定义IP核就是用vivado生成的带一个S_AXI_LITE的IP核。内部含有四个寄存器供写入和读取，分别是slv_reg0、slv_reg1、slv_reg2、slv_reg3。 接线只要在ZYNQ的M_AXI_GP0连接的AXI Interconnect添加一个Master口，再接在自定义IP核的S_AXI口就行了，用的是AXI_LITE协议。 程序设计12345678910overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')print(overlay.ip_dict)print(overlay.ip_dict.keys())my_ip = overlay.tao_axi_lite_ctrl_0my_ip.mmio.write(0x00, 1)my_ip.mmio.write(0x00 + 4, 2)my_ip.read(0x00)my_ip.read(0x00 + 4) 先是加载overlayoverlay = Overlay(&#39;/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit&#39;) 输出overlayd的ip_dict看看，有没自定义的IP核print(overlay.ip_dict)print(overlay.ip_dict.keys()) 创建自定义IP核的对象my_ip = overlay.tao_axi_lite_ctrl_0这里有很多要解释，之前已经说过了，会根据_IPMap类把ip_dict等字典封装好，输入overlay.tao_axi_lite_ctrl_0，会根据这个key，找出其对应的驱动driver，这里的driver’: ，因为我没有为这个IP核自定义驱动，所以会调用默认的DefaultIP来初始化，之前一直没理解驱动类别是怎么判定的，现在发现是在_TCL解析种识别并加入到对应的ip_dict中的，如果要自定义驱动的话，要在自定义驱动中加上bindto = [&#39;xilinx.com:ip:axi_dma:7.1&#39;]，才能识别自定义IP核并绑定到自定义的驱动，具体的信息是在TCL中，这个以后去实验，再介绍。 这样就可以知道是根据默认驱动DefaultIP来初始化的了，DefaultIP很简单，就是定义了一个mmio对象和读写mmio的函数。所以返回的my_ip其实是一个DefaultIP类的对象，我可以直接用类内定义的读函数，也可以通过类的属性my_ip.mmio（类内根据ip_dict定义的一个MMIO类对象），直接用mmio的读和写。1234my_ip.mmio.write(0x00, 1)my_ip.mmio.write(0x00 + 4, 2)my_ip.read(0x00)my_ip.read(0x00 + 4) 两个参数分别是偏置和值。注意传入的基地址表示的是IP核的slv_reg0寄存器，基地址+4表示slv_reg1寄存器，所以我这里是分别给两个寄存器写入值，再读出来。 运行结果12'axi_dma_0': &#123;'phys_addr': 1077936128, 'addr_range': 65536, 'type': 'xilinx.com:ip:axi_dma:7.1', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'axi_dma_0', 'driver': &lt;class 'pynq.lib.dma.DMA'&gt;&#125;'tao_axi_lite_ctrl_0': &#123;'phys_addr': 1136656384, 'addr_range': 65536, 'type': 'xilinx.com:user:tao_axi_lite_ctrl:1.0', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'tao_axi_lite_ctrl_0', 'driver': &lt;class 'pynq.overlay.DefaultIP'&gt;&#125;&#125; 正确识别出两个IP核信息，并且能正确输入和读取。]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AXI-LITE总线协议]]></title>
    <url>%2F2018%2F01%2F23%2FAXI-LITE%E6%80%BB%E7%BA%BF%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[前言AXI（Advanced eXtensible Interface）本是由ARM公司提出的一种总线协议，Xilinx从6系列的FPGA开始对AXI总线提供支持，此时AXI已经发展到了AXI4这个版本，所以当你用到Xilinx的软件的时候看到的都是“AIX4”的IP，如Vivado打包一个AXI IP的时候，看到的都是Create a new AXI4 peripheral。AXI总线有三种类型： AXI4：主要面向高性能地址映射通信的需求，是面向地址映射的接口，允许最大256轮的数据突发传输； AXI4-Stream：面向高速流数据传输；去掉了地址项，允许无限制的数据突发传输规模，之前DMA到FIFO间数据传输就是用的AXI-Stream。 AXI4-Lite：是一个轻量级的地址映射单次传输接口，占用很少的逻辑单元。主要用于命令的控制。今天主要分析这个。 详解信号组成 读地址通道：ARVALID, ARADDR, ARREADY信号； 读数据通道：RVALID, RDATA, RREADY, RRESP信号； 写地址通道：AWVALID，AWADDR, AWREADY信号； 写数据通道：WVALID, WDATA，WSTRB, WREADY信号； 写应答通道：BVALID, BRESP, BREADY信号； 系统通道：ACLK，ARESETN信号。 读地址信号都是以AR开头（A：address；R：read）写地址信号都是以AW开头（A：address；W：write）读数据信号都是以R开头（R：read）写数据信号都是以W开头（W：write）应答型号都是以B开头（B：back）AXI4-Stream总线的信号都是以 T 开头的 AXI 接口 AXI-GP接口：是通用的AXI接口，包括两个32位主设备接口和两个32位从设备接口，用过改接口可以访问PS中的片内外设。 AXI-HP接口：是高性能/带宽的标准的接口，PL模块作为主设备连接（从下图中箭头可以看出）。主要用于PL访问PS上的存储器（DDR和On-Chip RAM） AXI-ACP接口：是ARM多核架构下定义的一种接口，中文翻译为加速器一致性端口，用来管理DMA之类的不带缓存的AXI外设，PS端是Slave接口。 AXI 握手过程采用的是READY，VALID握手通信机制。Slave产生READY信号来指明已经准备好接受数据或控制信息。Master产生VALID信号表示发送数据有效。这样就会有下面三种情况：（都是在后一信号为高后的下一个上升沿开始传输） VALID先变高READY后变高 READY先变高VALID后变高 VALID和READY信号同时变高 AXI的五个通道，每个通道都有握手机制 读写时序图 AXI 代码这里是通过VIVADO生成AXI_LITE的IP产生的参考代码来分析的，这里只创建了一个S_AXI_LITE接口，为以后写自己控制的模块做准备。先看顶层模块的接口定义。 接口信号123456789101112131415161718192021input wire s_axi_aclk,input wire s_axi_aresetn,input wire [C_S_AXI_ADDR_WIDTH-1 : 0] s_axi_awaddr,input wire [2 : 0] s_axi_awprot,input wire s_axi_awvalid,output wire s_axi_awready,input wire [C_S_AXI_DATA_WIDTH-1 : 0] s_axi_wdata,input wire [(C_S_AXI_DATA_WIDTH/8)-1 : 0] s_axi_wstrb,input wire s_axi_wvalid,output wire s_axi_wready,output wire [1 : 0] s_axi_bresp,output wire s_axi_bvalid,input wire s_axi_bready,input wire [C_S_AXI_ADDR_WIDTH-1 : 0] s_axi_araddr,input wire [2 : 0] s_axi_arprot,input wire s_axi_arvalid,output wire s_axi_arready,output wire [C_S_AXI_DATA_WIDTH-1 : 0] s_axi_rdata,output wire [1 : 0] s_axi_rresp,output wire s_axi_rvalid,input wire s_axi_rready 下面详细介绍每个信号 读通道 ARVALID 读地址有效。此信号表明Master的地址和控制信息有效 RVALID 读数据有效。此信号表明Master的数据有效 ARADDR 读地址 RDATA 读数据 ARREADY 读地址准备好了。该信号指示Slave准备好接受一个地址和相关联的控制信号 RREADY 读数据准备好了。该信号指示Slave准备好接收数据 ARPROT 保护类型。这个信号表示该事务的特权和安全级别，并确定是否该事务是一个数据存取或指令的访问 RRESP 读取响应。这个信号表明读事务处理的状态。 写通道 AWVALID 写地址有效。这个信号表示Master的写地址和控制信息有效。 WVALID 写有效。这个信号表示Master的写数据和选通信号有效。 AWADDR 写地址 WDATA 写数据 AWREADY 写地址准备好了。该信号指示Slave准备好接受一个地址和相关联的控制信号 WREADY 写准备好了。该信号指示Slave可以接受写数据。 WSTRB 写选通。这个信号表明该字节通道持有效数据。每一bit对应WDATA一个字节 AWPROT 写通道保护类型。这个信号表示该事务的特权和安全级别，并确定是否该事务是一个数据存取或指令的访问 应答通道 BVALID 写响应有效。此信号表明写命令的有效写入响应。 BREADY 响应准备。该信号指示Master可以接受一个响应信号 BRESP 写响应。这个信号表示写事务处理的状态。 axi_awreadyassign S_AXI_AWREADY = axi_awready; 直接输出的地址写就绪需要S_AXI_AWVALID和S_AXI_WVALID都为高，既写地址有效，写数据有效时，拉高写地址就绪。aw_en表示写地址使能，表示地址已经好了，是后面的条件。1234567891011121314151617181920212223242526272829always @( posedge S_AXI_ACLK ) begin if ( S_AXI_ARESETN == 1'b0 ) begin axi_awready &lt;= 1'b0; aw_en &lt;= 1'b1; end else begin if (~axi_awready &amp;&amp; S_AXI_AWVALID &amp;&amp; S_AXI_WVALID &amp;&amp; aw_en) begin // slave is ready to accept write address when // there is a valid write address and write data // on the write address and data bus. This design // expects no outstanding transactions. axi_awready &lt;= 1'b1; aw_en &lt;= 1'b0; end else if (S_AXI_BREADY &amp;&amp; axi_bvalid) begin aw_en &lt;= 1'b1; axi_awready &lt;= 1'b0; end else begin axi_awready &lt;= 1'b0; end end end axi_awaddr用来锁存住地址，需要：S_AXI_AWVALID、S_AXI_WVALID、aw_en都为高。123456789101112131415always @( posedge S_AXI_ACLK ) begin if ( S_AXI_ARESETN == 1'b0 ) begin axi_awaddr &lt;= 0; end else begin if (~axi_awready &amp;&amp; S_AXI_AWVALID &amp;&amp; S_AXI_WVALID &amp;&amp; aw_en) begin // Write Address latching axi_awaddr &lt;= S_AXI_AWADDR; end end end axi_wreadyassign S_AXI_WREADY = axi_wready;输出的写数据准备接收就绪需要：S_AXI_AWVALID、S_AXI_WVALID、aw_en都为高12345678910111213141516171819202122always @( posedge S_AXI_ACLK ) begin if ( S_AXI_ARESETN == 1'b0 ) begin axi_wready &lt;= 1'b0; end else begin if (~axi_wready &amp;&amp; S_AXI_WVALID &amp;&amp; S_AXI_AWVALID &amp;&amp; aw_en ) begin // slave is ready to accept write data when // there is a valid write address and write data // on the write address and data bus. This design // expects no outstanding transactions. axi_wready &lt;= 1'b1; end else begin axi_wready &lt;= 1'b0; end end end PS写入到PL寄存器这部分实现内存映射寄存器的选择和写逻辑的生成。1assign slv_reg_wren = axi_wready &amp;&amp; S_AXI_WVALID &amp;&amp; axi_awready &amp;&amp; S_AXI_AWVALID; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051always @( posedge S_AXI_ACLK ) begin if ( S_AXI_ARESETN == 1'b0 ) begin slv_reg0 &lt;= 0; slv_reg1 &lt;= 0; slv_reg2 &lt;= 0; slv_reg3 &lt;= 0; end else begin if (slv_reg_wren) begin case ( axi_awaddr[ADDR_LSB+OPT_MEM_ADDR_BITS:ADDR_LSB] ) 2'h0: for ( byte_index = 0; byte_index &lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 0 slv_reg0[(byte_index*8) +: 8] &lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end 2'h1: for ( byte_index = 0; byte_index &lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 1 slv_reg1[(byte_index*8) +: 8] &lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end 2'h2: for ( byte_index = 0; byte_index &lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 2 slv_reg2[(byte_index*8) +: 8] &lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end 2'h3: for ( byte_index = 0; byte_index &lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin // Respective byte enables are asserted as per write strobes // Slave register 3 slv_reg3[(byte_index*8) +: 8] &lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end default : begin slv_reg0 &lt;= slv_reg0; slv_reg1 &lt;= slv_reg1; slv_reg2 &lt;= slv_reg2; slv_reg3 &lt;= slv_reg3; end endcase end end end 这段程序的作用是，当PS那边向AXI4-Lite总线写数据时，PL这边负责将数据接收到寄存器slv_reg。而slv_reg寄存器有0~3共4个。至于赋值给哪一个由axi_awaddr[ADDR_LSB+OPT_MEM_ADDR_BITS:ADDR_LSB]决定，根据宏定义其实就是由axi_awaddr[3:2]（写地址中不仅包含地址，而且包含了控制位，这里的[3:2]就是控制位）决定赋值给哪个slv_reg。 PS调用写函数时，如果不做地址偏移的话，axi_awaddr[3:2]的值默认是为0的，举个例子，如果我们自定义的IP的地址被映射为0x43C00000，那么我们mmio(0x43C00000,Value)写的就是slv_reg0的值。如果地址偏移4位，如mmio(0x43C00000 + 4,Value) 写的就是slv_reg1的值，依次类推。 下面只提取slv_reg0来说明，其他是类似的：12345for ( byte_index = 0; byte_index &lt;= (C_S_AXI_DATA_WIDTH/8)-1; byte_index = byte_index+1 ) if ( S_AXI_WSTRB[byte_index] == 1 ) begin slv_reg0[(byte_index*8) +: 8] &lt;= S_AXI_WDATA[(byte_index*8) +: 8]; end 其中，C_S_AXI_DATA_WIDTH的宏定义的值为32，也就是数据位宽，S_AXI_WSTRB就是写选通信号，S_AXI_WDATA就是写数据信号。当byte_index = 0的时候，就读取低8位，slv_reg0[7:0] &lt;= S_AXI_WDATA[7:0];，所以WSTRB控制着WDATA中哪些字节能够被写入寄存器。这样，我们只需要读取slv_reg0的值，就可以得到PS段写入的数据了。 axi_bvalid 、 axi_bresp写应答逻辑生成，直接输出到总线assign S_AXI_BRESP = axi_bresp;assign S_AXI_BVALID = axi_bvalid;axi_bvalid表示已将数据写入，axi_bresp回应的内容有4种见下表。 BRESP[1:0] Response Meaning b00 OKAY 正常通道传输成功 b01 EXOKAY 独特通道传输成功 b10 SLVERR 从机错误 b11 DECERR 解码错误 1234567891011121314151617181920212223242526always @( posedge S_AXI_ACLK ) begin if ( S_AXI_ARESETN == 1'b0 ) begin axi_bvalid &lt;= 0; axi_bresp &lt;= 2'b0; end else begin if (axi_awready &amp;&amp; S_AXI_AWVALID &amp;&amp; ~axi_bvalid &amp;&amp; axi_wready &amp;&amp; S_AXI_WVALID) begin // indicates a valid write response is available axi_bvalid &lt;= 1'b1; axi_bresp &lt;= 2'b0; // 'OKAY' response end // work error responses in future else begin if (S_AXI_BREADY &amp;&amp; axi_bvalid) //check if bready is asserted while bvalid is high) //(there is a possibility that bready is always asserted high) begin axi_bvalid &lt;= 1'b0; end end end end axi_arready 、 axi_arvalidassign S_AXI_ARREADY = axi_arready;地址读就绪assign S_AXI_RVALID = axi_rvalid;地址读有效，这些都类似，容易理解，不详细分析了。 PS读取PL寄存器先根据控制位选择读取哪个寄存器。根据axi_araddr[3:2]选择把哪个寄存器slv_reg赋值给输出寄存器reg_data_out。123456789101112assign slv_reg_rden = axi_arready &amp; S_AXI_ARVALID &amp; ~axi_rvalid;always @(*)begin // Address decoding for reading registers case ( axi_araddr[ADDR_LSB+OPT_MEM_ADDR_BITS:ADDR_LSB] ) 2'h0 : reg_data_out &lt;= slv_reg0; 2'h1 : reg_data_out &lt;= slv_reg1; 2'h2 : reg_data_out &lt;= slv_reg2; 2'h3 : reg_data_out &lt;= slv_reg3; default : reg_data_out &lt;= 0; endcaseend 然后把数据输出到总线1234567891011121314151617always @( posedge S_AXI_ACLK ) begin if ( S_AXI_ARESETN == 1'b0 ) begin axi_rdata &lt;= 0; end else begin // When there is a valid read address (S_AXI_ARVALID) with // acceptance of read address by the slave (axi_arready), // output the read dada if (slv_reg_rden) begin axi_rdata &lt;= reg_data_out; // register read data end end end 同样当PS调用读取函数时，这里axi_awaddr[3:2]默认是0，所以我们只需要把slv_reg0替换成我们自己数据，就可以让PS通过总线读到我们提供的数据。或者通过基地址+4来选择下一个寄存器。 目前这里只有4个寄存器，那是因为之前选择的是4个，其实我们可以定义的更多，在生成IP核的Number of Registers里选择。 重要的事情再说一遍如果我们自定义的IP的地址被映射为0x43C00000，那么用mmio(0x43C00000,Value)写的就是slv_reg0的值。如果地址偏移4位，如mmio(0x43C00000 + 4,Value) 写的就是slv_reg1的值。 补充一个对地址的理解地址编辑器种的offset Address应该是指基地址BASEADDR，high Address指的是最高地址。要求i是基地址加偏置不能超过最高地址。]]></content>
      <tags>
        <tag>PYNQ</tag>
        <tag>AXI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[控制多个IP核]]></title>
    <url>%2F2018%2F01%2F22%2F%E6%8E%A7%E5%88%B6%E5%A4%9A%E4%B8%AAIP%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[思路尝试用ZYNQ控制多个IP核，之前已经了解了AXI-interconnect的作用，可以通过它连接多个Master和Slaver，之前是通过ZYNQ两端接axi-interconnect连DMA的，现在尝试把两个axi-interconnect都增加Master和Slaver，再增加一个DMA串FIFO。接好时钟和复位。自动设置，配置好两个DMA的地址。开始测试。help(overlay)可以看到识别出了两个IP核都是DMA类型 axi_dma_0 : pynq.lib.dma.DMA axi_dma_1 : pynq.lib.dma.DMA 因为axi_dma_0接的是vivado提供的ip核AXI4-Stream DATA FIFO，而axi_dma_1接的是我自己写的FIFO，用如下代码分别创建两个dma对象，可以获得不同的输出，证明是达到控制不同IP核的效果。12dma = overlay.axi_dma_0dma1 = overlay.axi_dma_1 原理分析123456789101112overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')dma = overlay.axi_dma_0dma1 = overlay.axi_dma_1xlnk = Xlnk()in_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)out_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)dma.sendchannel.transfer(in_buffer)dma.recvchannel.transfer(out_buffer) dma.sendchannel.wait()dma.recvchannel.wait() 根据overlay解析里知道，首先调用Overlay类加载Su.bit，就可以解析对应的Su.tcl，根据_TCL类生成对象tcl。 保存tcl中的ip_dict、gpio_dict、interrupt_controllers、interrupt_pins、hierarchy_dict、clock_dict为overlay的类属性，因为有两个dma的ip核，所以ip_dict中应该包含以下部分：12&#123;axi_dma_0: &#123;'phys_addr' : 物理地址1, 'addr_range' : 地址范围1, 'type' : 'ip', 'config' : dict, 'state' : str, 'interrupts' : dict, 'gpio' : dict, 'fullpath' : str&#125;&#125;&#123;axi_dma_1: &#123;'phys_addr' : 物理地址2, 'addr_range' : 地址范围2, 'type' : 'ip', 'config' : dict, 'state' : str, 'interrupts' : dict, 'gpio' : dict, 'fullpath' : str&#125;&#125; 这里两个IP核根据地址不同就有区别了。 然后根据ip_dict 和 hierarchy_dict建立_IPMap类对象，作为overlay._ip_map，会新建两个属性axi_dma_0和axi_dma_0，值为对应的驱动描述。 执行dma = overlay.axi_dma_0时，根据其pynq.lib.dma.DMA，会构建对应的DMA类对象。12345678910111213141516class DMA(DefaultIP): def __init__(self, description, *args, **kwargs): if type(description) is not dict or args or kwargs: raise RuntimeError('You appear to want the old DMA driver which ' 'has been deprecated and moved to ' 'pynq.lib.deprecated') super().__init__(description=description) if 'mm2s_introut' in description['interrupts']: self.sendchannel = _DMAChannel(self.mmio, 0x0, self.mm2s_introut) else: self.sendchannel = _DMAChannel(self.mmio, 0x0) if 's2mm_introut' in description['interrupts']: self.recvchannel = _DMAChannel(self.mmio, 0x30, self.s2mm_introut) else: self.recvchannel = _DMAChannel(self.mmio, 0x30) 这里的super()是调用父类进行初始化，如果自己写的IP核的话一般就用默认的DefaultIP来初始化而不是DMA了，后面控制自己写的IP核可以留意下。下面这段是class DefaultIP被调用的初始化程序。123456789101112131415def __init__(self, description): self.mmio = MMIO(description['phys_addr'], description['addr_range']) if 'interrupts' in description: self._interrupts = description['interrupts'] else: self._interrupts = &#123;&#125; if 'gpio' in description: self._gpio = description['gpio'] else: self._gpio = &#123;&#125; for interrupt, details in self._interrupts.items(): setattr(self, interrupt, Interrupt(details['fullpath'])) for gpio, entry in self._gpio.items(): gpio_number = GPIO.get_gpio_pin(entry['index']) setattr(self, gpio, GPIO(gpio_number, 'out')) 可以看到根据phys_addr物理地址和addr_range地址范围构建了MMIO对象，分别作为mmio的基地址和长度了。这里就可以看出两个DMA的基地址已经不同了，因为是根据各自物理地址来确定的。 这样后面创建通道self.recvchannel = _DMAChannel(self.mmio, 0x30)就是跟据各自的mmio，基地址已经不同了，相同的偏置代表对应了各自同一个寄存器。所以后面的read和write操作中mmio读写的寄存器也是各自的，就达到了控制多个IP核的目的。 思考要控制自己的IP核还要想一下AXI-Lite总线体现在哪里，这里的话控制DMA的是S_AXI_LITE接口，猜测是MMIO是根据AXI_LITE总线读写的。]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYNQ之overlay]]></title>
    <url>%2F2018%2F01%2F21%2FPYNQ%E4%B9%8Boverlay%2F</url>
    <content type="text"><![CDATA[前言PYNQ板子分为PS（Processing System）和PL（Programmable Logic）部分，PS就是CPU，ARM Cortex-A9芯片，跑了Linux系统，可用JUpyter Notebooks打开。PL部分就是FPGA可编程逻辑，是ZYNQ-7020。而overlay可以让PS和PL部分连起来，提供了接口让PS的python可以直接调用PL的Overlay。比如把外围设备封装成python库，就可以直接控制硬件了。 overlay组成 .bit FPGA的比特流文件，就是平常烧录到板子里的 .tcl 框图的描述文件，根据BD设计图可用Vivado自动生成，在加载bit文件时会自动读取同一目录下的tcl文件，tcl文件必须和bit文件同名。 .py 这是python提供的接口，加载overlay后，可根据tcl或取各个IP核的物理地址等信息，通过读写寄存器封装成我们需要的功能。 几个注意点 Q：overlay是加载模块还是加载系统，能否有多个? A：overlay是动态加载的，也就是在加载每个overlay时下载对应的bit到FPGA并解析tcl文件，所以当前时刻只有一个overlay在用。 Q：overlay中FPGA里设计的引脚是怎么和python调用时的名字对应的？ A：通过tcl文件中提供的ip_dict、gpio_dict字典。 Overlay实现这个部分主要介绍一下overlay加载的overlay.py是怎么实现的，理解一下如何从tcl了中读取需要的信息，以及如何烧录程序等等，后面也会讲一下.bit和.tcl是怎么生成的。 overlay.py关键程序Overlay类这个类跟踪一个比特流的状态和内容。保存比特流的状态并启用运行保护绑定(应该是运行时不能用另外的Overlay)。我们对overlay的定义是比特流可配置设计，因此，该类必须通过内容的发现和运行时的保护来体现可配置性。overlay把IP和hierarchies层次结构体现为属性。如果没有定制IP驱动和层级驱动，就会用’DefaultIP’和’DefaultHierarchy’来构建。这个类储存了四种字典：IP、GPIO、interrupt controller、interrupt pin。 四个字典 IP：&#39;name&#39; -&gt; {phys_addr, addr_range, type, config, state} name (str)：是字典的key，也可以看作入口，直接用名字即可访问 phys_addr (int)：是IP核的物理地址 addr_range (int)：地址的范围 type (str)：IP核的类型 config (dict)：配置参数的字典 state (str)：IP核的状态信息 GPIO：&#39;name&#39; -&gt; {pin, state} pin (int)： 用户对GPIO的索引，从0开始 state (str)： GPIO的状态信息 Interrupt Controller：&#39;name&#39; -&gt; {parent, index} parent (str)： 父控制器的名字，如果直接连到PS7就为空’’ index (int)：中断的索引 Interrupt Pin：&#39;name&#39; -&gt; {controller, index} controller (str)：中断控制器的名字 index (int)：line线的索引 属性 bitfile_name : str .bit文件的绝对路径 bitstream : Bitstream 对应的.bit文件 ip_dict : dict PS7所有可寻址的IP核。上面介绍的IP字典 {str: {&#39;phys_addr&#39; : int, &#39;addr_range&#39; : int, &#39;type&#39; : str, &#39;config&#39; : dict, &#39;state&#39; : str}} gpio_dict : dict PS7控制的所有GPIO Pin. {str: {&#39;index&#39; : int, &#39;state&#39; : str}} interrupt_controllers : dict 连接到PS7中断线的所有AXI中断控制器. {str: {&#39;parent&#39;: str, &#39;index&#39; : int}} PS7是层次结构的根目录root，is未命名的。 interrupt_pins : dict 连接到中断控制器的所有Pin {str: {&#39;controller&#39; : str, &#39;index&#39; : int}} init12345678910111213141516171819def __init__(self, bitfile_name, download=True, ignore_version=False): # We need to be explicit here due to the way dynamic class # class construction interacts with super. Subclasses of # Overlay work correctly however. Bitstream.__init__(self, bitfile_name) tcl = _TCL(_get_tcl_name(self.bitfile_name)) self.ip_dict = tcl.ip_dict self.gpio_dict = tcl.gpio_dict self.interrupt_controllers = tcl.interrupt_controllers self.interrupt_pins = tcl.interrupt_pins self.hierarchy_dict = tcl.hierarchy_dict self.clock_dict = tcl.clock_dict description = _complete_description( self.ip_dict, self.hierarchy_dict, ignore_version) self._ip_map = _IPMap(description) if download: self.download() 返回一个新的Overlay对象，初始化一个.bit文件作为成员。需要vivado生成的同名.tcl文件放在相同目录下。_TCL是pl.py里解析.tcl文件的一个类，可以获取tcl的各种信息，比如ip_dict等，保存成overlay自己的信息。把ip_dict和hierarchy_dict封装成描述description，并根据该描述创建一个_IPMap（IP映射）类对象。download表示是否要重网上下载overlay。 关键的getattr12345678def __getattr__(self, key): """Overload of __getattr__ to return a driver for an IP or hierarchy. Throws an `RuntimeError` if the overlay is not loaded. """ if self.is_loaded(): return getattr(self._ip_map, key) else: raise RuntimeError("Overlay not currently loaded") attr就是属性，getattr是获取属性的方法。这里的self._ip_map是根据tcl获取的描述建立的_IPMap类对象，其中保存类ip、gpio等的描述，这样通过key就可以直接获取对应的属性了，其实_IPMap里也有一个getattr，输入key其实是在那里建立对应的IP的驱动，后面细讲。12overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')dma = overlay.axi_dma_0 上面例子中，axi_dma_0就是key，其对应的描述已经在建立Overlay对象overlay时就保存在属性_ip_map里了，这样直接执行overlay.axi_dma_0就会调用getattr方法，获取overlay._ip_map对应的属性（建立对应的DMA驱动）。 DefaultIP 类这是默认的IP驱动类，主要就是定义了一个mmio对象作为属性，然后包含read和write(对mmio里读写的简单封装)，用户自定义的驱动也要从这里继承，做自定义驱动的基类。提供了GPIO输出通道和中断输入属性。 属性 mmio : pynq.MMIO 设备的潜在MMIO驱动，MMIO是重之前讲的mmio.py导入的，可把外部设备映射为内存 _interrupts : dict 这个IP的PL.interrupt_pins的子集 _gpio : dict 这个IP的PL.gpio_dict的子集 init123456789101112131415def __init__(self, description): self.mmio = MMIO(description['phys_addr'], description['addr_range']) if 'interrupts' in description: self._interrupts = description['interrupts'] else: self._interrupts = &#123;&#125; if 'gpio' in description: self._gpio = description['gpio'] else: self._gpio = &#123;&#125; for interrupt, details in self._interrupts.items(): setattr(self, interrupt, Interrupt(details['fullpath'])) for gpio, entry in self._gpio.items(): gpio_number = GPIO.get_gpio_pin(entry['index']) setattr(self, gpio, GPIO(gpio_number, 'out')) 根据描述（包含了ip_dict 和 hierarchy_dict）中的物理地址创建MMIO类对象，把该IP核映射到内存上，并可以直接读写。并根据有无中断和GPIO创建对应的interrupt和gpio属性。 setattr(object, name, values) 给对象的属性赋值，若属性不存在，先创建再赋值。 read12def read(self, offset=0): return self.mmio.read(offset) 从IP核读取，offset（int）是基于基地址的偏置，就是要读取的位置。 write12def write(self, offset, value): self.mmio.write(offset, value) 向IP核写入，offset（int）是基于基地址的偏置，value是要写入的数据，可以为int或bytes。 _IPMap 类把IP, hierarches, interrupts, gpio的驱动保存为属性。 init12def __init__(self, desc): self._description = desc 根据描述创建一个新的_IPMap类对象。 类内getattr12345678910111213141516171819202122232425def __getattr__(self, key): if key in self._description['hierarchies']: hierdescription = self._description['hierarchies'][key] hierarchy = hierdescription['driver'](hierdescription) setattr(self, key, hierarchy) return hierarchy elif key in self._description['ip']: ipdescription = self._description['ip'][key] driver = ipdescription['driver'](ipdescription) setattr(self, key, driver) return driver elif key in self._description['interrupts']: interrupt = Interrupt( self._description['interrupts'][key]['fullpath']) setattr(self, key, interrupt) return interrupt elif key in self._description['gpio']: gpio_index = self._description['gpio'][key]['index'] gpio_number = GPIO.get_gpio_pin(gpio_index) gpio = GPIO(gpio_number, 'out') setattr(self, key, gpio) return gpio else: raise AttributeError( "Could not find IP or hierarchy &#123;&#125; in overlay".format(key)) 这里的key就是name，会根据key是属于IP, hierarches, interrupts, gpio中的哪一个，获取对应的驱动DefaultIP 和DefaultHierarchy（如果没有自定义的话）。比如key是axi_dma_0，可知是一个IP核的名字，在加载overlay是就会根据tcl生成的描述创建一个_IPMap类，其中解析到axi_dma_0时就会识别其是一个IP核，执行下面这一段代码：123ipdescription = self._description['ip'][key]driver = ipdescription['driver'](ipdescription)setattr(self, key, driver) 这里读取ip_dict的值可以看到包含这样一个键值对1'axi_dma_0': &#123;'phys_addr': 1077936128, 'addr_range': 65536, 'type': 'xilinx.com:ip:axi_dma:7.1', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'axi_dma_0', 'driver': &lt;class 'pynq.lib.dma.DMA'&gt;&#125; 在执行dma = overlay.axi_dma_0时，通过Overlay里的getattr，再执行_IPMap的getattr，先获取key对应的驱动信息pynq.lib.dma.DMA，然后新建一个名为key（这里axi_dma_0）的属性，并把相应的信息保存在里面，这里axi_dma_0对应的驱动应该是DMA（继承自DefaultIP），所以这里相当于创建了一个DMA类的对象保存在，axi_dma_0属性中，再把对象赋给dma。 setattr(object, name, values) 给对象的属性赋值，若属性不存在，先创建再赋值。 Overlay例子这是DAC比赛中给的例子，截取了部分进行分析。1234567891011overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')dma = overlay.axi_dma_0xlnk = Xlnk()in_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)out_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)dma.sendchannel.transfer(in_buffer)dma.recvchannel.transfer(out_buffer) dma.sendchannel.wait()dma.recvchannel.wait() 首先根据Su.bit文件建立Overlay类的对象overlay。从程序上分析： 调用Overlay的init进行初始化，生成overlay对象。传入的是Su.bit的绝对路径。 自动获取该路径下同名的Su.tcl文件，根据_TCL类生成对象，就是对tcl解析。 保存tcl中的ip_dict、gpio_dict、interrupt_controllers、interrupt_pins、hierarchy_dict、clock_dict为overlay的类属性，其中有一个dma的ip核，所以ip_dict中包含以下部分 1'axi_dma_0': &#123;'phys_addr': 1077936128, 'addr_range': 65536, 'type': 'xilinx.com:ip:axi_dma:7.1', 'state': None, 'interrupts': &#123;&#125;, 'gpio': &#123;&#125;, 'fullpath': 'axi_dma_0', 'driver': &lt;class 'pynq.lib.dma.DMA'&gt;&#125; 根据ip_dict 和 hierarchy_dict建立_IPMap类对象，作为overlay._ip_map dma = overlay.axi_dma_0就会调用overlay的getattr方法，进而调用_IPMap的getattr，获取驱动描述pynq.lib.dma.DMA，建立overlay._ip_map.axi_dma_0属性，根据驱动描述pynq.lib.dma.DMA类（继承自DefaultIP）创建对应的ip驱动，既生成一个DMA类的对象，赋给overlay._ip_map.axi_dma_0属性，最后把该对象赋给dma。到底是哪里调用了DefaultIP可以参考这个教程]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYNQ之mmio]]></title>
    <url>%2F2018%2F01%2F21%2FPYNQ%E4%B9%8Bmmio%2F</url>
    <content type="text"><![CDATA[MMIO概念MMIO（Memory-mapped I/O，内存映射I/O），与之对应的是PMIO（port-mapped I/O，端口映射I/O），是PC机在中央处理器（CPU）和外部设备之间执行输入输出操作的两种方法。简单点说就是直接把外部设备映射为内存，CPU就可以访问地址就可以直接访问外围设备，通过MMIO就可以用python来配置FPGA上的寄存器，来控制各个IP核了。 mmio.py这里主要根据PYNQ的mmio.py来看是怎么工作的，硬件上是ZYNQ芯片通过Axi-Lite总线控制其他IP核的。这里只有MMIO一个类，来做读和写，比较简单。 属性 virt_base : int 虚拟基地址，The address of the page for the MMIO base address. virt_offset : int 虚拟偏差，The offset of the MMIO base address from the virt_base. base_addr : int 基地址，The base address, not necessarily page aligned. length : int 地址范围的byte长度，The length in bytes of the address range. debug : bool 是否开启调试，Turn on debug mode if it is True. mmap_file : file Underlying file object for MMIO mapping mem : mmap 文件映射到内存时创建的对象，An mmap object created when mapping files to memory. array : numpy.ndarray A numpy view of the mapped range for efficient assignment init12345678910111213141516171819202122232425262728293031323334def __init__(self, base_addr, length=4, debug=False): if base_addr &lt; 0 or length &lt; 0: raise ValueError("Base address or length cannot be negative.") #返回当前进程的有效用户ID euid = os.geteuid() if euid != 0: raise EnvironmentError('Root permissions required.') # Align the base address with the pages self.virt_base = base_addr &amp; ~(mmap.PAGESIZE - 1) # Calculate base address offset w.r.t the base address self.virt_offset = base_addr - self.virt_base # Storing the base address and length self.base_addr = base_addr self.length = length self.debug = debug self._debug('MMIO(address, size) = (&#123;0:x&#125;, &#123;1:x&#125; bytes).', self.base_addr, self.length) # Open file and mmap self.mmap_file = os.open('/dev/mem', os.O_RDWR | os.O_SYNC) self.mem = mmap.mmap(self.mmap_file, self.length + self.virt_offset, mmap.MAP_SHARED, mmap.PROT_READ | mmap.PROT_WRITE, offset=self.virt_base) self.array = np.frombuffer(self.mem, np.uint32, length &gt;&gt; 2, self.virt_offset) 返回一个新的MMIO对象 参数 base_addr : int 基地址， length : int 长度，默认为4. debug : bool 是否开启调试， 默认是False. 实现流程 基地址和page对齐，作为虚拟基地址self.virt_base = base_addr &amp; ~(mmap.PAGESIZE - 1) 计算基地址和虚拟基地址的偏差，作为虚拟偏差self.virt_offset = base_addr - self.virt_base 储存基地址和长度 打开文件和mmapself.mmap_file = os.open(&#39;/dev/mem&#39;, os.O_RDWR | os.O_SYNC)/dev/mem是linux物理内存的全映像，可以用来访问物理内存，按上面这样打开后就可以用mmap来访问物理内存以及外设的IO资源。/dev/mem没那么简单 mmap（Memory-mapped）内存映射，一般用来将物理地址映射为虚拟地址 将数据内存转化为np数组self.array = np.frombuffer(self.mem, np.uint32, length &gt;&gt; 2, self.virt_offset) np.frombuffer 将mem内存转化成np的数组，接下来只要读写数组就相当于读写内存了 read1234567891011121314def read(self, offset=0, length=4): if length != 4: raise ValueError(&quot;MMIO currently only supports 4-byte reads.&quot;) if offset &lt; 0: raise ValueError(&quot;Offset cannot be negative.&quot;) idx = offset &gt;&gt; 2 if offset % 4: raise MemoryError(&apos;Unaligned read: offset must be multiple of 4.&apos;) self._debug(&apos;Reading &#123;0&#125; bytes from offset &#123;1:x&#125;&apos;, length, offset) # Read data out return int(self.array[idx]) 从MMIO读取数据 参数 offset : int 相对于基地址的偏移量，从这里读取 length : int 数据分割的长度 idx = offset &gt;&gt; 2右移两位相当于除以4，因为是按4字节长度存成数组的，所以索引要除以4。 write123456789101112131415161718192021def write(self, offset, data): if offset &lt; 0: raise ValueError(&quot;Offset cannot be negative.&quot;) idx = offset &gt;&gt; 2 if offset % 4: raise MemoryError(&apos;Unaligned write: offset must be multiple of 4.&apos;) if type(data) is int: self._debug(&apos;Writing 4 bytes to offset &#123;0:x&#125;: &#123;1:x&#125;&apos;, offset, data) self.array[idx] = np.uint32(data) elif type(data) is bytes: length = len(data) num_words = length &gt;&gt; 2 if length % 4: raise MemoryError(&apos;Unaligned write: data length must be multiple of 4.&apos;) buf = np.frombuffer(data, np.uint32, num_words, 0) self.array[idx:idx + num_words] = buf else: raise ValueError(&quot;Data type must be int or bytes.&quot;) 写入数据到MIMO 参数 offset : int 相对于基地址的偏移量，从这里写入 data : int / bytes 要写入到MMIO的数据，分成int整型和bytes字节型]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYNQ之AXI-Interconnect]]></title>
    <url>%2F2018%2F01%2F21%2FPYNQ%E4%B9%8BAXI-Interconnect%2F</url>
    <content type="text"><![CDATA[介绍AXI协议严格的讲是一个点对点的主从接口协议，当多个外设需要互相交互数据时，我们需要加入一个AXI Interconnect模块（AXI互联），作用是提供将一个或多个AXI主设备连接到一个或多个AXI从设备的一种交换机制。包含多个LogicCORE IP核实例（infrastructure cores，基础核），会自动引入infrastructure cores来让任意AXI主设备和AXI从设备连接连接。后来感觉不需要单独写一篇来介绍，只需要理解为可以连接多个主从就行了，比如需要用PYNQ控制多个IP核，类似分线器的功能。 AXI Infrastructure Cores AXI Crossbar 将一个或者多个相似的内存映射的主设备连接到一个或者多个相似的内 存映射的从设备。 AXI Data Width Converter将一个内存映射的主设备连接到一个数据位宽不同的内存 映射的从设备。 AXI Clock Converter 将一个内存映射的主设备连接到一个不同时钟域的内存映射的 从设备。 AXI Protocol Converter 将AXI4、AXI3或者AXI4-Lite协议的主设备连接到不同AXI协 议的内存映射从设备。 AXI Data FIFO 在内存映射的主设备与从设备之间连接一些FIFO缓存。 AXI Register Slice 在内存映射的主从设备之间插入一组并行的寄存器，典型目的是为了打断关键路径。 AXI MMU 为AXI互联模块提供地址范围译码和设备从映射服务。 因为我要研究怎么控制多个IP核，这里只介绍一下我需要用到的几个cores。 AXI Crossbar 只有在多个Master或多个Slave时才会用到。（最多可以有16个M和16个S） 可选互联架构（Selectable Interconnect Architecture） Crossbar mode（性能最优） Shared-Address, Multiple-Data （SAMD共享地址多路数据） 读写数据通道都是并行的crossbar路径。当多个读或写数据源需要传输的时候，数据可以彼此独立、并行的传输。 根据配置连接映射来减少crossbar 数据路径，来减少资源的占用 共享的写地址仲裁器, 加上一个共享的读地址仲裁器。 只有在AXI互联模块被配置被AXI4或者AXI3 协议时，Crossbar模式才有效。 Shared Access mode （面积最优） 共享的读数据路径,共享的写数据路径和一个共享的读些地址路径 一次传输仅支持一个事务，使用资源最少 AXI Protocol Converter AXI4、AXI3 到 AXI4-Lite 的协议转换 AXI4 到 AXI3 的协议转换 使用模式 N-to-1 interconnect 多个主设备仲裁访问一个从设备，比如内存控制器 1-to-N interconnect 一个主设备访问多个从设备，比如处理器访问多个内存映射 N-to-M interconnect(Crossbar Mode) 特点是共享内存，多路数据 M-to-N interconnect(Shared Access Mode)]]></content>
      <tags>
        <tag>PYNQ</tag>
        <tag>AXI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PYNQ之dma]]></title>
    <url>%2F2018%2F01%2F19%2FPYNQ%E4%B9%8Bdma%2F</url>
    <content type="text"><![CDATA[前言正如ZYNQ是用C来配置FPGA的寄存器，PYNQ是用python配置寄存器来控制FPGA上模块的。这里主要介绍lib/dma.py，分析如何控制DMA IP核工作的dma.py在lib里，在pynq初始化时直接import了lib包，就根据该目录下的_init_.py把dma导入了。 _init_.py文件相当于类的初始化函数把所在的目录当作一个packageimport时直接导入整个package DMA主要类介绍1. DMA用于和AXI DMA核交互的，是整个DMA调用的接口。包含两个属性，读通道和写通道，两个通道都是用_DMAChannel类来建立的，因此有相同的API，都含有transfer和wait函数。用来传输的buffer必须是由xlnk用cma_array建立的，为了保证物理地址的连续。 属性 recvchannel : _DMAChannel 从AXI-stream读取通过DMA搬到系统内存 sendchannel : _DMAChannel 从系统内存读取通过DMA般到AXI-streaminit123456789101112131415def __init__(self, description, *args, **kwargs): if type(description) is not dict or args or kwargs: raise RuntimeError('You appear to want the old DMA driver which ' 'has been deprecated and moved to ' 'pynq.lib.deprecated') super().__init__(description=description) if 'mm2s_introut' in description['interrupts']: self.sendchannel = _DMAChannel(self.mmio, 0x0, self.mm2s_introut) else: self.sendchannel = _DMAChannel(self.mmio, 0x0) if 's2mm_introut' in description['interrupts']: self.recvchannel = _DMAChannel(self.mmio, 0x30, self.s2mm_introut) else: self.recvchannel = _DMAChannel(self.mmio, 0x30) 这个类只有一个函数init，初始化自身，创建一个DMA实例。 参数 description : dict 描述DMA engine入口的IP dict，是在加载overlay时从tcl中获取的，以IP核的名字为key，所以直接用名字就可以初始化，创建DMA实例，dma = overlay.axi_dma_0。这里理解还不是很清楚，看完overlay再改 ip dict说明key：IP名字value：物理地址physical address, 地址范围address range, IP类型IP type, 配置字典configuration dictionary, 相关声明state associated{str: {‘phys_addr’ : int, ‘addr_range’ : int, ‘type’ : str, ‘config’ : dict, ‘state’ : str}}. 这里分别建了两个通道，sendchannel和recvchannel，作为发送和接收。 sendchannel： 基址为0x00，从手册中可以看出对应着MM2S的寄存器（Memory Map to Stream），既从内存到AXI-stream recvchannel： 基址为0x30，从手册中可以看出对应着S2MM的寄存器（Stream to Memory Map），既从AXI-stream到内存 这里的super().__init__是指用其父类DefaultIP来初始化。 2. _DMAChannelDMA通道类，驱动一个AXi DMA的信号通道，用来连接Xlnk.cma_array（分配内存的方法）通道的主要函数transfer和wait用来开始和等待各自的传输结束。如果有中断，才会用wait_async来协同。该类不能直接构造，要通过AxiDMA class来构造。 init12345def __init__(self, mmio, offset, interrupt=None): self._mmio = mmio self._offset = offset self._interrupt = interrupt self.start() 参数 _mmio： 内存映射I\O，把外围设备映射为内存，让PS直接读写寄存器，达到控制设备的目的。 _offset： 偏置 _interrupt： 有无中断 在类的方法或属性前加一个“_”单下划线，意味着该方法或属性不应该去调用，它并不属于API。 running123@propertydef running(self): return self._mmio.read(self._offset + 4) &amp; 0x01 == 0x00 当DMA engine正在运行时为ture。例如：发送通道offset为0x00，根据手册知道0x00是MM2S_DMACR控制寄存器（Memory Map to Stream Register），0x04是MM2S_DMASR状态寄存器，&amp;0x01表示取其最低位Halted，0表示运行，1表示停止。 @property 可以将python定义的函数“当做”属性访问 idle123@propertydef idle(self): return self._mmio.read(self._offset + 4) &amp; 0x02 == 0x02 当DMA engine闲置的时候为ture，transfer只有在DMA闲置的时候才能被访问。例如：接收通道offset为0x30，根据手册知道0x34是S2MM_DMASR状态寄存器，&amp;0x02取其第二位Idle，1表示闲置，0表示不闲置。 start12345678def start(self): if self._interrupt: self._mmio.write(self._offset, 0x1001) else: self._mmio.write(self._offset, 0x0001) while not self.running: pass self._first_transfer = True 开启DMA engine。看发送通道无中断的情况，发送通道offset为0x00，根据手册知道0x00是MM2S_DMACR控制寄存器，0x0001表示最低位写入1，既RS位（Run/Stop），0表示stop，1表示run。在初始化最后会调用start()函数，开启DMA。 stop1234def stop(self): self._mmio.write(self._offset, 0x0000) while self.running: pass 终止当前传输，并停止DMA通道。和start类似，MM2S_DMACR控制寄存器最低位置0即可。 clear_interrupt12def _clear_interrupt(self): self._mmio.write(self._offset + 4, 0x1000) 清中断0x04是MM2S_DMASR状态寄存器，0x1000既第13位置1，由手册知道Dly_Irq位，虽然是状态寄存器，但说给这个位写1可以清除。 transfer12345678def transfer(self, array): if not self.running: raise RuntimeError('DMA channel not started') if not self.idle and not self._first_transfer: raise RuntimeError('DMA channel not idle') self._mmio.write(self._offset + 0x18, array.physical_address) self._mmio.write(self._offset + 0x28, array.nbytes) self._first_transfer = False 通过DMA传输，只有当DMA闲置是才能调用。 参数 array : ContiguousArray xlnk分配的数组 发送通道而言，0x18是MM2S_SA地址寄存器（MM2S DMA Source Address Register），32位，提供系统内存的地址，从PS内存读取到DMA中，在发送到AXI stream。这里是把物理地址赋给它。 Indicates the source address AXI DMA reads from to transfer data to AXI4-Stream on the MM2S Channel. 0x28是MM2S_LENGTH传输长度寄存器，提供传输的长度，从系统内存读取输送到AXI stream。0 ~ 22位是传输的比特数。23 ~ 31是保留位，写入也没有效果。只要配置好地址和传输长度DMA就会自己工作，而不需要CPU干预。 wait12345def wait(self): if not self.running: raise RuntimeError('DMA channel not started') while not self.idle: pass 等待传输完成，就是不停检测idle寄存器的值。 3. LegacyDMApython用来控制DMA的类，好像并没有用到，估计是要自己控制的话再实例化。用到了cffi（C Foreign Function Interface调用c的一个模块），不细讲这个类，到时再改。 属性 buf : cffi.FFI.CData 物理连续内存的指针，A pointer to physically contiguous buffer. bufLength : int 内置buffer的长度，Length of internal buffer in bytes. phyAddress : int DMA设备的物理地址，Physical address of the DMA device. DMAengine : cdata &#39;XAxiDma *&#39; DMA引擎实例，不能直接修改，DMA engine instance defined in C. Not to be directly modified. DMAinstance : cdata &#39;XAxiDma_Config *&#39; DMA配置实例结构，DMA configuration instance struct. Not to be directly modified. direction : int 传输方向，The direction indicating whether DMA sends/receives data from PL. Configuration : dict 当前DMA实例配置变量，Current DMAinstance configuration values. init123456789101112131415def __init__(self, address, direction=DMA_FROM_DEV, attr_dict=None): self.buf = None self.direction = direction self.bufLength = None self.phyAddress = address self.DMAengine = self.ffi.new("XAxiDma *") self.DMAinstance = self.ffi.new("XAxiDma_Config *") self.Configuration = &#123;&#125; self._gen_config(address, direction, attr_dict) status = self.libdma.XAxiDma_CfgInitialize(self.DMAengine, self.DMAinstance) if status != 0: raise RuntimeError("Failed to initialize DMA!") self.libdma.XAxiDma_Reset(self.DMAengine) self.libdma.DisableInterruptsAll(self.DMAengine) 参数 address: int DMA IP核的物理地址 direction : int 传输方向 DMA_TO_DEV : DMA发送到PL. DMA_FROM_DEV : DMA从 PL接收 DMA_BIDIRECTIONAL : 发送接收 attr_dict : dict 指定DMA配置的可选字典，An optional dictionary specifying DMA configuration values. _gen_config123456789101112131415161718192021222324252627def _gen_config(self, address, direction, attr_dict): for key in self.DefaultConfig.keys(): self.DMAinstance.__setattr__(key, self.DefaultConfig[key]) if direction == self.DMA_TO_DEV: self.DMAinstance.HasS2Mm = 0 self.DMAinstance.HasMm2S = 1 elif direction == self.DMA_BIDIRECTIONAL: self.DMAinstance.HasS2Mm = 1 self.DMAinstance.HasMm2S = 1 self._bufPtr = None self._TransferInitiated = 0 if attr_dict is not None: if type(attr_dict) == dict: for key in attr_dict.keys(): self.DMAinstance.__setattr__(key, attr_dict[key]) else: print("Warning: Expecting 3rd Arg to be a dict.") virt = self.libxlnk.cma_mmap(address, 0x10000) if virt == -1: raise RuntimeError("Memory map of driver failed.") self.DMAinstance.BaseAddr = self.ffi.cast("uint32_t *", virt) self.DMAinstance.DeviceId = self.DeviceId self.DeviceId += 1 for key in self.DefaultConfig.keys(): self.Configuration[key] = self.DMAinstance.__getattribute__(key) 建立配置，映射内存，初始化方法，不需要用户调用。 transfer1234567891011121314151617181920212223def transfer(self, num_bytes=-1, direction=-1): if num_bytes == -1: num_bytes = self.bufLength if direction == -1: direction = self.direction if num_bytes &gt; self.bufLength: raise RuntimeError("Buffer size smaller than the transfer size") if num_bytes &gt; self.DMA_TRANSFER_LIMIT_BYTES: raise RuntimeError("DMA transfer size &gt; &#123;&#125;".format( self.DMA_TRANSFER_LIMIT_BYTES)) if direction not in [self.DMA_FROM_DEV, self.DMA_TO_DEV]: raise RuntimeError("Invalid direction for transfer.") self.direction = direction if self.buf is not None: self.libdma.XAxiDma_SimpleTransfer( self.DMAengine, self._bufPtr, num_bytes, self.direction ) self._TransferInitiated = 1 else: raise RuntimeError("Buffer not allocated.") 用于发起物理地址连续的buffer和PL之间的数据传输，buffer要通过create_buf函数创建或者get_ndarray 参数 num_bytes : int 传输的比特数 direction : int 传输方向 create_buf分配物理地址连续的内存 free_buf释放对象关联的内存 wait阻塞直到DMA busy或者timeout get_bufGet a CFFI pointer to object’s internal buffer get_ndarray从DMA buffer获取一个numpy ndarry configure重新配置并重新初始化IP核 使用实例不是完整的程序，只说明关键步骤。12345678910overlay = Overlay('/home/xilinx/jupyter_notebooks/DAC/overlay/Su.bit')dma = overlay.axi_dma_0xlnk = Xlnk()in_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)out_buffer = xlnk.cma_array(shape=(4,), dtype=np.uint32)dma.sendchannel.transfer(in_buffer)dma.recvchannel.transfer(out_buffer) dma.sendchannel.wait()dma.recvchannel.wait() 建立DMA实例dma = overlay.axi_dma_0这里有点复杂，再后面的overlay里说明，可以简单理解为根据axi_dma_0的驱动描述pynq.lib.dma.DMA建立了一个DMA类对象赋给dma。 建立buffer因为DMA搬运只能是物理地址连续的，所以只能用xlnk来建立buffer。 传输数据实例化了dma就会有两个通道属性。调用transfer函数设置地址和长度并开始传输，wait等待传输完成进入idle状态。12345678def transfer(self, array): if not self.running: raise RuntimeError('DMA channel not started') if not self.idle and not self._first_transfer: raise RuntimeError('DMA channel not idle') self._mmio.write(self._offset + 0x18, array.physical_address) self._mmio.write(self._offset + 0x28, array.nbytes) self._first_transfer = False 这里可以看到，分别把in_buffer.physical_address赋值给地址寄存器，把in_buffer.nbytes赋值给长度寄存器，这样就确定的了搬运的起始位置和长度，只需等待DMA搬完就可以了。 xlnk是放入一个值到buffer相当于拷贝到指定的连续内存，nbytes记录着buffer里的数据个数。]]></content>
      <tags>
        <tag>PYNQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown语法]]></title>
    <url>%2F2018%2F01%2F18%2FMarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[标题Markdown 支持两种标题的语法，Setext 和 atx 形式。 Setext ：底线形式，利用 = (最高阶)和 - (第二阶)， Atx ：行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶。 修辞和强调 斜体: * * 加粗: ** ** 删除线: ~~ ~~ 引用在文本前加入 &gt; (大于号) 这里是引用 列表 无序列表: *、+、- 有序列表: 数字. 图片与链接 图片：![]() 链接：[]() 代码int n = 0;代码框：用 扩起来 代码块：上下加三个 表格123| Table1 | Table2 | Table3 || ------ | ------ | ------ || 1 | 2 | 3 | Table1 Table2 Table3 1 2 3 换行末尾敲两个空格再回车 分割线三个*号 数学公式 Latex 语法 行内公式：单美元符号包围$ $ 块公式：双美元符号包围$$ $$ 上下标 公式 markdown $y^2$ y^2 $x_2$ x_2 分数 公式 markdown $\frac{x}{y}$ \frac{x}{y} 开根号 公式 markdown $\sqrt{x}$ \sqrt{x} $\sqrt[n]{x}$ \sqrt[n]{x} 矢量 公式 markdown $\vec{a}$ \vec{a} 积分 公式 markdown $\int^2_3{\rm d}x$ \int^2_3{\rm d}x 极限 公式 markdown $\lim_{n\rightarrow\infty}$ lim_{n\rightarrow\infty} 累加、累乘 公式 markdown $\sum$ \sum $\prod$ \prod 希腊字母 大写 markdown 小写 markdown $A$ A $\alpha$ \alpha $B$ B $\beta$ \beta $\Gamma$ \Gamma $\gamma$ \gamma $\Delta$ \Delta $\delta$ \delta 待续未完 关系运算符 运算符 markdown $\pm$ \pm $\times$ \times $\div$ \div $\neq$ \neq $\leq$ \leq $\geq$ \geq]]></content>
      <tags>
        <tag>日常技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F16%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
